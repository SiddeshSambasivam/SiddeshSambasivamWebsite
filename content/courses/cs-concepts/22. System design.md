---
title: "22. System Design"
date: "2022-08-21"
type: book
weight: 22
---

{{< toc >}}

## A. Introduction

The three stages to the interview:

- **Problem:** The interviewer gives you a purposely vague task.
- **Clarification:** You show leadership and experience by asking meaningful questions to clarify requirements and performance priorities.
- **Presentation:** You give an impromptu presentation on solutions (probably as youâ€™re thinking of the solution.)

## B. Methodology - Framework for System Design

1. Always start with **clarification**!

   - Ask about the priorities, scale and restrictions of the system

     - How many users?
     - How much data?
     - How many requests per second?
     - Latency requirements?

2. After understanding the requirements, start with a **high level solution**

   - Start abstract and simple to lay out an end to end solution
   - Draw an abstract diagram of the system
   - the components should be technology agnostic and abstract away scaling and concrete methods
   - eg: Checking whether the system prediction is real time, pre calculated batch or some hybrid

3. Start filling in some details; **architectural components**

   - eg: How models are delivered to the product? What type of infrastructure is required?

     - Decribe the reasoning behind the design choices

   - add data models if necessary

4. Start with **concerete component implementations**

   - Discuss the tradeoffs of the design choices
   - eg: Why did you choose a particular database? Why did you choose a particular language?

5. Finishing touches

## C. ML Model Development

1. Providing wide array of model types for the problem; it's good to cover some breadth instead of naming one solution.

2. Offline training and evaluation

   - Data used for training
     - How to deal with data imbalances?
   - Metrics used to compare models- Precision, Recall, F1 Score, ROC AUC, etc.
   - Evaluation methods
     - K-fold cross validation
     - Training sample selection

3. Online Evaluation

   - Evaluate the model performance via A/B testing
     - What metrics to use?

4. Model lifecycle management
   - Monitoring the model to ensure its health
   - Operations to track to keep model performance
     - How to manage the model lifecycle?
     - How to deal with model drift?
     - How to deal with model decay?

### I. ML Operational Diffculties

#### A. ML Drifts

{{< figure src="/leetcode/Diagram-1-min-1024x424.png" title="ML Drifts" >}}

1. Concept drift - The learned distribution changes over time due to behavioural changes
2. Data drift - The learned distribution changes over time due to changes in the data

#### What to do about it?

1. Setup data integrity and outlier monitoring
   - Data errors slowly degrade
   - Missing values
   - Broken data pipelines due to bugs or API updates
2. Setup model drift monitoring
